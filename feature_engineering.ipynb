{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "980a888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the grouped list for colnames\n",
    "from utils import *\n",
    "#目前只有mary road有tc,我们在做union model时候先不用tc\n",
    "#可以单独给mary road做一个individual模型，里面使用tc\n",
    "weathers_2.remove('tc')\n",
    "\n",
    "# air_pollutions, weathers, weathers_2, times\n",
    "####\n",
    "# continuous_variables, category_variables, percentile_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a7200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74530b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'D:/Work/污染项目/data-overall-update/' \\\n",
    "              'data_process/before_engineer/union_per_pollution/'\n",
    "\n",
    "output_folder = 'D:/Work/污染项目/data-overall-update/' \\\n",
    "              'data_process/after_engineer/union_per_pollution/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d879b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['nox', 'no2', 'no', 'o3', 'pm2.5', 'pm10'],\n",
       " ['ws', 'wd', 'temp', 'RH'],\n",
       " ['ssr', 'tp', 'blh', 'tcc', 'sp'],\n",
       " ['date_unix', 'week', 'weekday', 'hour', 'month', 'day_julian'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_pollutions, weathers, weathers_2, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f41f144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ws',\n",
       "  'wd',\n",
       "  'temp',\n",
       "  'RH',\n",
       "  'ssr',\n",
       "  'tp',\n",
       "  'blh',\n",
       "  'tcc',\n",
       "  'sp',\n",
       "  'ap_lat',\n",
       "  'ap_long',\n",
       "  'met_lat',\n",
       "  'met_long',\n",
       "  'date_unix',\n",
       "  'year'],\n",
       " ['ap_code',\n",
       "  'MetCode',\n",
       "  'site',\n",
       "  'week',\n",
       "  'weekday',\n",
       "  'hour',\n",
       "  'month',\n",
       "  'day_julian'],\n",
       " ['ws', 'wd', 'temp', 'RH', 'ssr', 'tp', 'blh', 'tcc', 'sp'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuous_variables, category_variables, percentile_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d0769c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnull_ratio(df, target_colname):\n",
    "    null_cnts =  df[df[target_colname].isnull()].shape[0]\n",
    "    return null_cnts/df.shape[0]\n",
    "\n",
    "# from date unix extract year\n",
    "def extract_year(row):\n",
    "    date_unix = int(row['date_unix'])\n",
    "    return datetime.utcfromtimestamp(date_unix).year\n",
    "\n",
    "def fill_continuous_missValues(air_pollution,\n",
    "                               data_folder,\n",
    "                               n_fold):\n",
    "\n",
    "    df = pd.read_csv(data_folder+air_pollution+'_unionSites.csv')\n",
    "    \n",
    "    #add column year\n",
    "    df['year'] = df.apply(lambda row : extract_year(row), axis=1)\n",
    "\n",
    "    #the sites related for this pollution\n",
    "    related_sites = df.ap_code.unique()\n",
    "    \n",
    "    #missing values的行，直接用那一列在那个站点的中位数代替就可以\n",
    "    no_missing_dfs = []\n",
    "\n",
    "    for site in related_sites:\n",
    "\n",
    "        df_site = df[df['ap_code']==site]\n",
    "\n",
    "        for colname in continuous_variables:\n",
    "            df_site[colname] = df_site[colname].fillna(df_site[colname].median())\n",
    "        \n",
    "        #ids for cross validation at each site level\n",
    "        ids = list(range(df_site.shape[0]))\n",
    "        random.shuffle(ids)\n",
    "        ids = [item % n_fold for item in ids]\n",
    "        df_site['cv_fold'] = ids\n",
    "\n",
    "        no_missing_dfs.append(df_site)\n",
    "\n",
    "    df = pd.concat(no_missing_dfs)\n",
    "    return df\n",
    "\n",
    "def percentiles_return(df, percentile_variables):\n",
    "    \n",
    "    percentiles = [25, 50, 75]\n",
    "    row_cnt = df.shape[0]\n",
    "    df_percentile = []\n",
    "    df_percentile_columns = []\n",
    "    \n",
    "    for colname in percentile_variables:\n",
    "        for percentile in percentiles:\n",
    "            df_percentile.append([np.percentile(df[colname], percentile)]*row_cnt)\n",
    "            df_percentile_columns.append(colname+'_percentile'+str(percentile))\n",
    "            \n",
    "    return pd.DataFrame(df_percentile, columns=df_percentile_columns)\n",
    "        \n",
    "    \n",
    "\n",
    "#get and save all percentiles at ap_code level into a dictionary\n",
    "def getNsave_percentiles_dict(df, \n",
    "                              percentile_variables, \n",
    "                              air_pollution, \n",
    "                              output_folder):\n",
    "    #save all percentiles at ap_code level into a dictionary\n",
    "\n",
    "    percentiles_dict = {}\n",
    "    percentiles = [25, 50, 75]\n",
    "\n",
    "    for colname in percentile_variables:\n",
    "        percentiles_dict[colname]={}\n",
    "        for ap_code in df.ap_code.unique():\n",
    "            df_code = df[df['ap_code']==ap_code]\n",
    "            percentiles_dict[colname][ap_code] = {\n",
    "                '25': np.percentile(df_code[colname], 25),\n",
    "                '50': np.percentile(df_code[colname], 50),\n",
    "                '75': np.percentile(df_code[colname], 75)\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "    # Serializing json\n",
    "    json_object = json.dumps(percentiles_dict, indent=4)\n",
    "\n",
    "    # Writing to sample.json\n",
    "    with open(output_folder+air_pollution+\"_percentiles_dict_ap_code_lvl.json\", \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "    \n",
    "    return percentiles_dict\n",
    "\n",
    "\n",
    "# get the percentile for specific column at ap_code level\n",
    "def col_percentile(row, colname, percentile, percentiles_dict):\n",
    "    ap_code = row['ap_code']\n",
    "    return percentiles_dict[colname][ap_code][percentile]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da5afe",
   "metadata": {},
   "source": [
    "# 1. 填补missing values用median填补\n",
    "# 2. 加上cross validation column\n",
    "# 3. 加上新的features: get and save all percentiles for each column at 25,50,75 at ap_code level(这个先不用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82eac367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rxt29\\AppData\\Local\\Temp/ipykernel_14412/4221650816.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_site[colname] = df_site[colname].fillna(df_site[colname].median())\n",
      "C:\\Users\\rxt29\\AppData\\Local\\Temp/ipykernel_14412/4221650816.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_site['cv_fold'] = ids\n"
     ]
    }
   ],
   "source": [
    "for air_pollution in air_pollutions:\n",
    "    #fillin missing values and add cross validation column\n",
    "    df = fill_continuous_missValues(air_pollution,\n",
    "                                    data_folder,\n",
    "                                    n_fold = 5)\n",
    "    \n",
    "\n",
    "#     percentiles_dict = getNsave_percentiles_dict(df, \n",
    "#                                   percentile_variables, \n",
    "#                                   air_pollution, \n",
    "#                                   \"percentiles/\")\n",
    "\n",
    "\n",
    "#     #col_percentile(row, colname, percentile, percentiles_dict):\n",
    "\n",
    "#     for colname in percentile_variables:\n",
    "#         for percentile in ['25', '50', '75']:\n",
    "#             df[colname+'_'+percentile] = df.apply(lambda row : col_percentile(row, \n",
    "#                                                                               colname, \n",
    "#                                                                               percentile, \n",
    "#                                                                               percentiles_dict), axis=1)\n",
    "\n",
    "\n",
    "        \n",
    "    df.to_csv(output_folder+air_pollution+'_unionSites.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0598804d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
