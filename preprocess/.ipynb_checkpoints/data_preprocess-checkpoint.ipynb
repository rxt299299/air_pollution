{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8548151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2fb7758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"D:/work/GitHub/air_pollution/\")\n",
    "#load the grouped list for colnames\n",
    "from utils import *\n",
    "# air_pollutions, weathers, weathers_2, times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d2c0f",
   "metadata": {},
   "source": [
    "# 将每个站点的x和y的数据分别存在两个单独的csv里面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "191c5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrate the intersection\n",
    "# of two lists in most simple way\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "\n",
    "#write the x and y csv for each site\n",
    "def write_xy_files(site_name, df_site, all_x_columns, all_y_columns, output_folder):\n",
    "    df = pd.read_csv(data_folder+site_name+'.csv')\n",
    "    \n",
    "    df['index'] = df.index\n",
    "    #extract the included x features and y features\n",
    "    df_x = df[['index']+intersection(['code']+all_x_columns, df.columns)]\n",
    "    df_y = df[['index']+intersection(all_y_columns, df.columns)]\n",
    "    #rename the code column for the later merge with site info\n",
    "    df_x = df_x.rename(columns={\"code\": \"ap_code\"})\n",
    "    \n",
    "    #merge x dataframe with the site info\n",
    "    df_x = df_x.merge(df_site, how='left', on=['ap_code'])\n",
    "    \n",
    "    df_x.to_csv(output_folder+site_name+'_x.csv', index=False)\n",
    "    df_y.to_csv(output_folder+site_name+'_y.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04d0cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"D:/work/污染项目/data-overall-update-0120/data/\"\n",
    "site_info_path = \"D:/work/污染项目/data-overall-update-0120/siteinfo.csv\"\n",
    "output_folder = \"D:/work/污染项目/data-overall-update-0120/data_process/before_engineer/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11340208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ap_code</th>\n",
       "      <th>ap_lat</th>\n",
       "      <th>ap_long</th>\n",
       "      <th>MetCode</th>\n",
       "      <th>met_lat</th>\n",
       "      <th>met_long</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MY1</td>\n",
       "      <td>51.522530</td>\n",
       "      <td>-0.154611</td>\n",
       "      <td>37720</td>\n",
       "      <td>51.479167</td>\n",
       "      <td>-0.450556</td>\n",
       "      <td>UT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KC1</td>\n",
       "      <td>51.521050</td>\n",
       "      <td>-0.213492</td>\n",
       "      <td>37720</td>\n",
       "      <td>51.479167</td>\n",
       "      <td>-0.450556</td>\n",
       "      <td>UB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BIRR</td>\n",
       "      <td>52.476140</td>\n",
       "      <td>-1.874978</td>\n",
       "      <td>35340</td>\n",
       "      <td>52.453889</td>\n",
       "      <td>-1.748056</td>\n",
       "      <td>UT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BRS8</td>\n",
       "      <td>51.462839</td>\n",
       "      <td>-2.584482</td>\n",
       "      <td>37243</td>\n",
       "      <td>51.382778</td>\n",
       "      <td>-2.719167</td>\n",
       "      <td>UB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LED6</td>\n",
       "      <td>53.819970</td>\n",
       "      <td>-1.576361</td>\n",
       "      <td>33463</td>\n",
       "      <td>53.611667</td>\n",
       "      <td>-1.666944</td>\n",
       "      <td>UT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LEED</td>\n",
       "      <td>53.803780</td>\n",
       "      <td>-1.546472</td>\n",
       "      <td>33463</td>\n",
       "      <td>53.611667</td>\n",
       "      <td>-1.666944</td>\n",
       "      <td>UB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ABD7</td>\n",
       "      <td>57.144550</td>\n",
       "      <td>-2.106472</td>\n",
       "      <td>30910</td>\n",
       "      <td>57.205000</td>\n",
       "      <td>-2.205278</td>\n",
       "      <td>UT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AH</td>\n",
       "      <td>52.503850</td>\n",
       "      <td>-3.034178</td>\n",
       "      <td>35200</td>\n",
       "      <td>52.243056</td>\n",
       "      <td>-2.885833</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SCN2</td>\n",
       "      <td>53.586340</td>\n",
       "      <td>-0.636811</td>\n",
       "      <td>33735</td>\n",
       "      <td>53.306944</td>\n",
       "      <td>-0.548056</td>\n",
       "      <td>UI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>YK11</td>\n",
       "      <td>53.951890</td>\n",
       "      <td>-1.075861</td>\n",
       "      <td>32660</td>\n",
       "      <td>54.043591</td>\n",
       "      <td>-1.250316</td>\n",
       "      <td>UT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ap_code     ap_lat   ap_long  MetCode    met_lat  met_long site\n",
       "0     MY1  51.522530 -0.154611    37720  51.479167 -0.450556   UT\n",
       "1     KC1  51.521050 -0.213492    37720  51.479167 -0.450556   UB\n",
       "2    BIRR  52.476140 -1.874978    35340  52.453889 -1.748056   UT\n",
       "3    BRS8  51.462839 -2.584482    37243  51.382778 -2.719167   UB\n",
       "4    LED6  53.819970 -1.576361    33463  53.611667 -1.666944   UT\n",
       "5    LEED  53.803780 -1.546472    33463  53.611667 -1.666944   UB\n",
       "6    ABD7  57.144550 -2.106472    30910  57.205000 -2.205278   UT\n",
       "7      AH  52.503850 -3.034178    35200  52.243056 -2.885833   RB\n",
       "8    SCN2  53.586340 -0.636811    33735  53.306944 -0.548056   UI\n",
       "9    YK11  53.951890 -1.075861    32660  54.043591 -1.250316   UT"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the site info csv with info for each site\n",
    "df_site = pd.read_csv(site_info_path)\n",
    "#drop the Site and met columns, cause we already have their code column\n",
    "df_site = df_site.drop(['Site', 'met'], axis=1)\n",
    "df_site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dce0655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all x columns: \n",
      "['ws', 'wd', 'temp', 'RH', 'ssr', 'tp', 'blh', 'tcc', 'sp', 'tc', 'date_unix', 'week', 'weekday', 'hour', 'month', 'day_julian']\n",
      "all y columns: \n",
      "['nox', 'no2', 'no', 'o3', 'pm2.5', 'pm10']\n"
     ]
    }
   ],
   "source": [
    "#x, y data for each site\n",
    "csv_files = os.listdir(data_folder)\n",
    "csv_files = [item for item in csv_files if item.endswith('.csv')]\n",
    "\n",
    "all_x_columns = weathers+weathers_2+times\n",
    "all_y_columns = air_pollutions\n",
    "\n",
    "print (\"all x columns: \")\n",
    "print (all_x_columns)\n",
    "print (\"all y columns: \")\n",
    "print (all_y_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69bda6b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/work/污染项目/data-overall-update-0120/data_process/before_engineer/AberdeenUnionStreetRoadside_x.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-dbbf61583864>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msite_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msite_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msite_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     write_xy_files(site_name, \n\u001b[0m\u001b[0;32m      5\u001b[0m                    \u001b[0mdf_site\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                    \u001b[0mall_x_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-5a824421f501>\u001b[0m in \u001b[0;36mwrite_xy_files\u001b[1;34m(site_name, df_site, all_x_columns, all_y_columns, output_folder)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mdf_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_site\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ap_code'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mdf_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msite_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_x.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mdf_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msite_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_y.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3385\u001b[0m         )\n\u001b[0;32m   3386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3387\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         )\n\u001b[1;32m-> 1083\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \"\"\"\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/work/污染项目/data-overall-update-0120/data_process/before_engineer/AberdeenUnionStreetRoadside_x.csv'"
     ]
    }
   ],
   "source": [
    "#output the x and y files for each site\n",
    "for site_name in csv_files:\n",
    "    site_name = site_name[:-4]\n",
    "    write_xy_files(site_name, \n",
    "                   df_site, \n",
    "                   all_x_columns, \n",
    "                   all_y_columns, \n",
    "                   output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f876650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last step, 检查是否除了tc以外，其他的columns每个站点都有？\n",
    "x_files = os.listdir(output_folder)\n",
    "x_files = [item for item in x_files if item.endswith('_x.csv')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecf009f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tc not in columns in AberdeenUnionStreetRoadside\n",
      "tc not in columns in AstonHill\n",
      "tc not in columns in BirminghamA4540\n",
      "tc not in columns in BristolStPauls\n",
      "tc not in columns in LeedsCentre\n",
      "tc not in columns in LeedsHeadingleyKerbside\n",
      "tc not in columns in NorthKensington\n",
      "tc not in columns in ScunthorpeTown\n",
      "tc not in columns in YorkFishergate\n"
     ]
    }
   ],
   "source": [
    "for x_file in x_files:\n",
    "    site_name = x_file[:-6]\n",
    "    df_x = pd.read_csv(output_folder+x_file)\n",
    "    colnames = df_x.columns\n",
    "    \n",
    "    for col in all_x_columns+['index']:\n",
    "        if col not in colnames:\n",
    "            print (col+\" not in columns in \"+site_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffda177",
   "metadata": {},
   "source": [
    "# 将各个站点的数据union在一起，即一个y一个大的csv,包含所有站点的数据（先不考虑tc这个column）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fbff1921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unoin_x_y_df(x_y_folder, \n",
    "                 site_name, \n",
    "                 selected_x_columns, \n",
    "                 air_pollution):\n",
    "    #read x and y dataframes\n",
    "    df_x = pd.read_csv(x_y_folder+site_name+'_x.csv')\n",
    "    df_y = pd.read_csv(x_y_folder+site_name+'_y.csv')\n",
    "\n",
    "    #selected columns in x and y dataframes\n",
    "    df_x = df_x[selected_x_columns]\n",
    "    df_y = df_y[['index']+[air_pollution]]\n",
    "\n",
    "    #merge x dataframe with y use the index\n",
    "    df = df_y.merge(df_x, how='left', on=['index'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def union_df_perPollution(air_pollution, \n",
    "                          pollutions_include_sites, \n",
    "                          selected_x_columns,\n",
    "                          x_y_folder,\n",
    "                          union_output_path):\n",
    "    included_sites = pollutions_include_sites[air_pollution]\n",
    "    \n",
    "    df_list = []\n",
    "    for site_name in included_sites:\n",
    "        # x csv 和 y csv 合并的时候用index去join\n",
    "        df_list.append(unoin_x_y_df(x_y_folder, \n",
    "                                    site_name, \n",
    "                                    selected_x_columns, \n",
    "                                    air_pollution))\n",
    "        \n",
    "    df_union = pd.concat(df_list)\n",
    "    #filter out NA ws column and NA y \n",
    "    #######################################################\n",
    "    ### follow Grange and Carslaw原始论文里, 在训练模型前 ###\n",
    "    ### 只提取了有wind speed 的所有数据进行训练 ##############\n",
    "    #######################################################\n",
    "    df_union = df_union[~pd.isnull(df_union['ws'])]\n",
    "    df_union = df_union[~pd.isnull(df_union[air_pollution])]\n",
    "    \n",
    "    df_union.to_csv(union_output_path+air_pollution+'_unionSites.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "78a84473",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_output_path = \"D:/Work/污染项目/data-overall-update/\"\\\n",
    "                     \"data_process/before_engineer/union_per_pollution/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0b8eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_files = os.listdir(output_folder)\n",
    "y_files = [item for item in y_files if item.endswith('_y.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbeed0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#存储每个y(pollution)都包含哪一些站点(sites)\n",
    "pollutions_include_sites = defaultdict(list)\n",
    "\n",
    "for y_file in y_files:\n",
    "    site_name = y_file[:-6]\n",
    "    \n",
    "    df_y = pd.read_csv(output_folder+y_file)\n",
    "    y_colnames = df_y.columns\n",
    "    \n",
    "    for col in y_colnames:\n",
    "        if col != 'index':\n",
    "            pollutions_include_sites[col].append(site_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "811d095f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['nox', 'no2', 'no', 'o3', 'pm2.5', 'pm10']),\n",
       " ['nox', 'no2', 'no', 'o3', 'pm2.5', 'pm10'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pollutions_include_sites.keys(), air_pollutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35674a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_x_columns for training\n",
    "selected_x_columns = all_x_columns.copy()\n",
    "selected_x_columns.remove('tc')\n",
    "selected_x_columns = ['index','ap_code']+selected_x_columns+['ap_lat', 'ap_long', \n",
    "                                                     'MetCode', 'met_lat', \n",
    "                                                     'met_long', 'site']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5b608474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the unioned pollution csv\n",
    "for air_pollution in pollutions_include_sites.keys():\n",
    "    union_df_perPollution(air_pollution, \n",
    "                          pollutions_include_sites, \n",
    "                          selected_x_columns,\n",
    "                          output_folder,\n",
    "                          union_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8441f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
